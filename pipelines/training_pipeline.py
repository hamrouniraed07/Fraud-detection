"""Pipeline ZenML pour l'entra√Ænement."""
from zenml import pipeline, step
from zenml.config import DockerSettings
from zenml.integrations.mlflow.experiment_trackers import MLFlowExperimentTracker
import pandas as pd
import numpy as np
from typing import Tuple, Dict
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import joblib
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent))


@step
def load_data_step() -> Tuple[pd.DataFrame, pd.Series]:
    """
    Charge les donn√©es brutes.
    
    Returns:
        Tuple (X, y)
    """
    from src.data.load_data import load_raw_data, split_features_target
    
    print("üìÇ Chargement des donn√©es...")
    df = load_raw_data()
    X, y = split_features_target(df)
    
    return X, y


@step
def preprocess_data_step(
    X: pd.DataFrame, 
    y: pd.Series,
    test_size: float = 0.2
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    Pr√©traite et divise les donn√©es.
    
    Args:
        X: Features
        y: Target
        test_size: Proportion du test set
        
    Returns:
        Tuple (X_train, X_test, y_train, y_test)
    """
    from src.data.preprocess import preprocess_data
    
    print("‚öôÔ∏è  Pr√©traitement des donn√©es...")
    X_train, X_test, y_train, y_test = preprocess_data(X, y, test_size=test_size)
    
    return X_train, X_test, y_train, y_test


@step(experiment_tracker="mlflow_tracker")
def train_model_step(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    params: Dict
) -> RandomForestClassifier:
    """
    Entra√Æne le mod√®le.
    
    Args:
        X_train: Features d'entra√Ænement
        y_train: Target d'entra√Ænement
        params: Param√®tres du mod√®le
        
    Returns:
        Mod√®le entra√Æn√©
    """
    import mlflow
    
    print("üèãÔ∏è  Entra√Ænement du mod√®le...")
    
    # Logger les param√®tres
    mlflow.log_params(params)
    mlflow.log_param("train_samples", len(X_train))
    
    # Cr√©er et entra√Æner le mod√®le
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    
    print("‚úÖ Mod√®le entra√Æn√©!")
    return model


@step(experiment_tracker="mlflow_tracker")
def evaluate_model_step(
    model: RandomForestClassifier,
    X_test: pd.DataFrame,
    y_test: pd.Series
) -> Dict[str, float]:
    """
    √âvalue le mod√®le.
    
    Args:
        model: Mod√®le entra√Æn√©
        X_test: Features de test
        y_test: Target de test
        
    Returns:
        Dictionnaire des m√©triques
    """
    import mlflow
    
    print("üìä √âvaluation du mod√®le...")
    
    # Pr√©dictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    # Calculer les m√©triques
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1_score': f1_score(y_test, y_pred)
    }
    
    # Logger les m√©triques
    for metric_name, value in metrics.items():
        mlflow.log_metric(metric_name, value)
    
    print(f"‚úÖ F1-Score: {metrics['f1_score']:.4f}")
    
    return metrics


@step
def save_model_step(
    model: RandomForestClassifier,
    metrics: Dict[str, float],
    model_name: str = "fraud_model",
    threshold: float = 0.80
) -> str:
    """
    Sauvegarde le mod√®le si les m√©triques sont satisfaisantes.
    
    Args:
        model: Mod√®le entra√Æn√©
        metrics: M√©triques d'√©valuation
        model_name: Nom du mod√®le
        threshold: Seuil de F1-score pour sauvegarder
        
    Returns:
        Chemin du mod√®le sauvegard√©
    """
    import mlflow
    from datetime import datetime
    
    f1 = metrics['f1_score']
    
    if f1 >= threshold:
        # Cr√©er un nom avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = f"models/{model_name}_{timestamp}.pkl"
        
        # Sauvegarder
        joblib.dump(model, model_path)
        print(f"üíæ Mod√®le sauvegard√©: {model_path}")
        
        # Logger dans MLflow
        mlflow.sklearn.log_model(model, "model")
        
        return model_path
    else:
        print(f"‚ö†Ô∏è  F1-Score ({f1:.4f}) < seuil ({threshold}). Mod√®le non sauvegard√©.")
        return "not_saved"


@pipeline
def fraud_detection_training_pipeline(
    test_size: float = 0.2,
    params: Dict = None
):
    """
    Pipeline complet d'entra√Ænement.
    
    Args:
        test_size: Proportion du test set
        params: Param√®tres du mod√®le
    """
    # Param√®tres par d√©faut
    if params is None:
        params = {
            'n_estimators': 100,
            'max_depth': 10,
            'random_state': 42,
            'class_weight': 'balanced',
            'n_jobs': -1
        }
    
    # √âtapes du pipeline
    X, y = load_data_step()
    X_train, X_test, y_train, y_test = preprocess_data_step(X, y, test_size)
    model = train_model_step(X_train, y_train, params)
    metrics = evaluate_model_step(model, X_test, y_test)
    model_path = save_model_step(model, metrics)
    
    return model_path


def run_pipeline_with_config(config_name: str = "baseline"):
    """
    Lance le pipeline avec une configuration sp√©cifique.
    
    Args:
        config_name: Nom de la configuration dans model_params.yaml
    """
    import yaml
    
    # Charger les param√®tres
    with open('configs/model_params.yaml', 'r') as f:
        all_params = yaml.safe_load(f)
    
    params = all_params[config_name]
    
    print("\n" + "="*60)
    print(f"ZENML PIPELINE: {config_name.upper()}")
    print("="*60)
    print(f"Param√®tres: {params}")
    print("="*60 + "\n")
    
    # Lancer le pipeline
    pipeline_instance = fraud_detection_training_pipeline(params=params)
    pipeline_instance.run()


if __name__ == "__main__":
    """
    Pour ex√©cuter ce pipeline, d'abord:
    1. Initialiser ZenML: zenml init
    2. Configurer MLflow: zenml integration install mlflow
    3. Enregistrer le tracker: zenml experiment-tracker register mlflow_tracker --flavor=mlflow
    4. D√©finir la stack: zenml stack register mlflow_stack -o default -a default -e mlflow_tracker
    5. Activer la stack: zenml stack set mlflow_stack
    """
    
    # Lancer plusieurs pipelines
    print("üöÄ Lancement des pipelines ZenML...\n")
    
    # Pipeline baseline
    run_pipeline_with_config("baseline")
    
    # Pipeline variation 1
    # run_pipeline_with_config("variation_1")
    
    # Pipeline variation 2
    # run_pipeline_with_config("variation_2")